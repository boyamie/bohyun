---
{"dg-publish":true,"permalink":"/4-projects//"}
---

# 사전질문 1
### **1. 오디오 데이터 경량화**

#### **1.1. 고차계수 연산 생략**

MFCC(Mel-Frequency Cepstral Coefficients)에서 고차 계수는 보통 신호의 세부적인 텍스처를 설명하지만, 일부 작업에서는 고차 계수 생략이 성능에 큰 영향을 미치지 않습니다. 이를 통해 데이터 크기를 줄이고 처리 속도를 높일 수 있습니다.
- MFCC 추출 시 고차 계수를 포함하지 않고, 주요 성분(예: 첫 13차 계수)만 사용.
- 차원 축소로 계산 복잡도 감소 및 모델 처리 최적화.

#### **1.2. 정규화**

오디오 데이터는 모델이 다룰 수 있는 범위로 정규화해야 합니다. 정규화는 데이터 분포를 안정화하고 학습 수렴을 촉진합니다.
- 전체 데이터 세트의 RMS(root mean square)를 기준으로 신호를 정규화.
- dB 단위로 변환하여 상대적 에너지를 표현.
- 클리핑 및 노이즈 제거로 아웃라이어 데이터 제거.

#### **1.3. 세그먼트 분할**

오디오 데이터를 작은 세그먼트로 나누어 훈련 데이터를 더 풍부하게 만들 수 있습니다. 이는 데이터 양을 늘리고 모델 훈련에 필요한 메모리를 줄이는 데 유리합니다.

- 고정 길이로 분할(예: 1~2초 단위).
- Overlapping Windowing 기법으로 데이터 중복성을 활용해 정보 손실 최소화.

---

### **2. 오디오 데이터 증강 기법**

#### **2.1. Noise Injection**

데이터 다양성을 늘리고 모델의 일반화 성능을 높이기 위해 배경 소음을 추가하는 방식입니다.

- Gaussian Noise, Pink Noise 등을 추가.
- 다양한 SNR(Signal-to-Noise Ratio) 조건에서 증강된 데이터를 생성.

#### **2.2. Time Stretching**

오디오 데이터를 일정 비율로 늘리거나 줄여 시간적 변화를 모사합니다.

- STFT(Short-Time Fourier Transform)를 사용해 시간 스케일 변경.
- 과도한 왜곡을 방지하기 위해 ±10% 이내로 조정.

#### **2.3. Pitch Shifting**

음조를 조정하여 다양한 음역대를 모사합니다.

- Mel-spectrogram이나 MFCC 생성 이전에 샘플 속도를 조정.
- 음높이 변화 후 원래 시간 길이로 조정.

#### **2.4. Room Impulse Response (RIR) 적용**

실제 환경에서의 음향 특성을 반영하기 위해 인공적으로 잔향을 추가합니다.

- RIR 데이터셋 사용(예: 다양한 실내 환경을 반영한 IR 데이터).
- 훈련 시 정규화하여 지나친 잔향이 학습에 방해되지 않도록 조정.

#### **2.5. SpecAugment**

Mel-spectrogram에 대해 시간 및 주파수 차원에서 마스킹을 적용하는 기법입니다.

- 주파수 대역 마스킹(Frequency Masking): 특정 주파수 영역을 무작위로 가림.
- 시간 대역 마스킹(Time Masking): 오디오의 특정 시간 간격을 무작위로 가림.
- Time Warping: 시간 축을 약간 왜곡하여 변형.

#### **2.6. Volume Perturbation**

오디오 신호의 볼륨을 임의로 조정하여 다양한 녹음 환경을 반영합니다.

- ±3~5dB 범위로 볼륨을 무작위 변경.
- 원래 신호의 특징이 손상되지 않도록 주의.

#### **2.7. Mixing**

서로 다른 두 오디오 샘플을 혼합하여 새로운 데이터를 생성합니다.

- 샘플 간 가중치를 달리하여 혼합.
- 결과 신호의 에너지를 정규화.

---

### **3. 데이터 전처리에서 AMP 적용 및 MFCC로의 변경**

#### **3.1. AMP (Amplitude Modulation Processing) 적용**

- Amplitude Modulation을 활용하여 오디오의 에너지를 기반으로 신호를 강조하거나 약화합니다.
- **효과**: 중요한 음향 정보가 강조되고, 불필요한 잡음이 감소.

#### **3.2. Mel Spectrogram → MFCC로 변경 검토**

- **Mel-spectrogram의 단점**: 주파수 대역 정보를 효과적으로 보존하지만, 음성 특징 추출 측면에서는 MFCC보다 세밀하지 않을 수 있음.
- **MFCC의 장점**: 음향의 심리음향학적 특성 반영. 음성 관련 작업에 특히 유리.
- **변경 전략**
    - 기존 Mel-spectrogram 파이프라인을 MFCC 추출 파이프라인으로 대체.
    - 테스트 데이터에서 MFCC의 효과를 비교하여 변경 여부 결정.

---

### **4. 경량 모델의 데이터 전처리 및 증강 활용**

Salmonn 모델과 같은 경량 모델은 대규모 데이터셋에서 학습된 Foundation 모델을 기반으로 하기 때문에, 데이터 전처리와 증강 전략은 경량화된 모델에서도 중요한 성능 개선 요소입니다.

- **학습 데이터 경량화**: Stage1 및 Stage2 데이터를 중심으로 핵심 데이터만 사용.
- **증강 기법 활용**: 전처리를 최소화했지만, 추가적인 증강을 통해 경량 모델의 다양성을 보완.

---
# 사전질문 2
### **1. 오디오 데이터 경량화 기법**

#### **1.1. 샘플링 레이트 최적화**

오디오 데이터의 샘플링 레이트를 조정하면 데이터 크기를 줄이고 계산량을 낮출 수 있음. 예를 들어, 일반적인 음성 인식 작업에서는 16kHz 샘플링 레이트가 충분히 좋은 성능을 제공.

- 데이터가 44.1kHz와 같은 고해상도라면 16kHz로 다운샘플링.
- 디지털 필터를 사용해 샘플링 변경 과정에서의 품질 손실 최소화.

#### **1.2. 길이 조정 및 세그먼트화**

오디오 데이터의 길이가 불필요하게 긴 경우 학습 속도를 낮출 수 있음. 데이터 세그먼트화로 처리 속도를 높이고 다양한 샘플을 생성.

- 고정 길이로 자르기(예: 1~2초).
- Overlapping Windowing 기법으로 데이터 중복 활용.
- 긴 오디오에서 의미 없는 구간(무음)을 제거해 데이터 크기를 줄임.

#### **1.3. 압축 및 차원 축소**

MFCC(Mel-Frequency Cepstral Coefficients)를 활용하면 오디오 데이터의 차원을 대폭 줄이면서 주요 특성을 보존 가능.

- MFCC의 저차원 계수(예: 첫 13개)만 사용.
- 고차 계수를 생략해 정보의 과적합 가능성을 줄임.

#### **1.4. 양자화 (Quantization)**

오디오 데이터를 더 낮은 정밀도로 변환해 메모리 사용량을 줄이고 처리 속도를 향상.

- 16비트 오디오 데이터를 8비트 또는 4비트 정밀도로 변환.
- 양자화 후 신호 품질 유지 여부를 실험적으로 검증.

#### **1.5. 노이즈 제거 및 클리핑**

데이터에 포함된 불필요한 노이즈나 클리핑된 신호를 제거하면 처리 효율성을 높일 수 있음.

- Bandpass Filtering으로 주요 주파수 대역만 보존.
- 무음 구간을 제거하고 신호 대 노이즈 비율(SNR)을 최적화.

---

### **2. 오디오 데이터 증강 기법**

#### **2.1. Noise Injection**

배경 소음이나 Gaussian Noise를 추가해 모델의 일반화 성능을 강화.

- 데이터에 Gaussian Noise를 추가해 다양한 환경을 시뮬레이션.
- SNR(Signal-to-Noise Ratio)을 조정하며 실험.

#### **2.2. Pitch Shifting**

오디오의 음조를 변경해 모델이 다양한 음역대를 학습하도록 유도.

- 샘플 속도를 조정해 음높이를 ±1~2 반음 변화.
- 변형 후 원래의 신호 길이를 유지.

#### **2.3. Time Stretching**

시간 스케일을 조정해 신호 길이를 늘리거나 줄임.

- Time Stretching 기법으로 ±10%의 속도 변화 적용.
- STFT(Short-Time Fourier Transform) 기반으로 왜곡 최소화.

#### **2.4. SpecAugment**

Mel-spectrogram에서 특정 영역을 마스킹해 모델의 일반화 성능을 향상.

- Frequency Masking: 특정 주파수 대역을 무작위로 가림.
- Time Masking: 특정 시간 대역을 무작위로 가림.
- Time Warping: 시간 축을 왜곡해 변형.

#### **2.5. Room Impulse Response (RIR) 적용**

실제 환경에서의 잔향 효과를 반영해 현실적인 학습 데이터를 생성.

- RIR 데이터셋을 사용해 잔향을 추가.
- 실내, 야외 등 다양한 환경 조건을 반영.

#### **2.6. Mixing**

서로 다른 오디오 샘플을 혼합해 새로운 데이터 샘플을 생성.

- 두 샘플 간 가중치를 다르게 설정해 혼합.
- 혼합 후 신호의 에너지 정규화.

#### **2.7. Volume Perturbation**

볼륨을 무작위로 조정해 다양한 녹음 환경을 모사.

- ±3~5dB 범위에서 볼륨 조정.
- 신호 특성이 유지되도록 과도한 조정 방지.

---

### **3. 가설 검증과 경량화 실험 환경에의 적용**

#### **3.1. 경량화 전략**

- **고성능 모델 경량화**:
    - **Quantization-aware Training**: 양자화를 염두에 둔 훈련으로 성능 저하 최소화.
    - **Iterative Pruning**: 중요하지 않은 뉴런을 반복적으로 제거하면서 재학습.
- **저성능 모델 성능 개선**:
    - **Pruning + Re-train**: 단순히 모델을 압축하기보다 재학습으로 성능 개선.
    - **Data Augmentation 활용**: 부족한 데이터의 다양성을 증대해 성능 보완.

#### **3.2. 성능 평가 지표**

- **WER (Word Error Rate)**: 음성 인식 정확도를 평가하기 위한 표준 지표.
- **SPIDEr**: 오디오 캡셔닝과 같은 복잡한 멀티모달 task에서 사용되는 지표로, BLEU, CIDEr 등의 종합 지표를 활용.

#### **3.3. 실험 환경 설정**

- 모델 경량화 후 **latency**, **memory usage**, **text generation 결과**를 평가.
- **리더보드 제출 전 사전 검증**:
    - 모델이 불필요하게 긴 텍스트를 생성하지 않도록 디버깅.
    - 반복 생성 방지: Beam Search, Sampling 등 최적화 기법 활용.

# 사전질문 3
### **1. 오디오 데이터 경량화 기법**

#### **1.1. 샘플링 레이트 최적화**

오디오 데이터를 모델에 적합하게 만들기 위해 샘플링 레이트를 조정하여 데이터 크기를 줄이고 처리 속도를 높입니다.

- **적용 방안**:
    - 일반적인 음성 인식 작업에서는 16kHz 샘플링 레이트가 적합.
    - 고해상도 오디오(예: 44.1kHz)는 다운샘플링을 통해 데이터 크기를 줄이고 계산량 감소.
    - 리샘플링 시 품질 유지 필터 적용.

#### **1.2. 신호 길이 조정**

오디오 데이터의 불필요하게 긴 길이는 모델 처리에 부담을 줄 수 있으므로 길이를 조정하거나 세그먼트로 나눕니다.

- **적용 방안**:
    - 일정 길이(예: 1~2초)로 오디오 데이터를 분할.
    - Overlapping Windowing 기법을 사용해 중복 데이터를 생성하여 정보 손실 최소화.
    - 무음 구간 제거로 데이터를 간소화.

#### **1.3. 차원 축소**

오디오 데이터를 모델이 처리하기 쉽게 차원을 줄입니다.

- **적용 방안**:
    - MFCC(Mel-Frequency Cepstral Coefficients)를 사용하여 음성의 주요 특성만 추출.
    - 낮은 차원의 MFCC 계수(예: 첫 13개)만 사용하여 처리 효율성을 높임.

#### **1.4. 압축 및 양자화**

오디오 데이터를 압축하거나 양자화하여 모델 학습에 필요한 메모리와 계산량을 줄입니다.

- **적용 방안**:
    - 오디오 데이터를 16비트에서 8비트 또는 4비트로 양자화.
    - 양자화 후 음질 저하 여부를 검증하여 최적화.

#### **1.5. 노이즈 제거**

데이터에 포함된 불필요한 노이즈나 클리핑 신호를 제거하여 학습 데이터의 품질을 향상.

- **적용 방안**:
    - Bandpass Filtering을 사용해 특정 주파수 대역의 신호만 보존.
    - SNR(Signal-to-Noise Ratio)을 최적화하여 신호 품질을 개선.

---

### **2. 오디오 데이터 증강 기법**

#### **2.1. Noise Injection**

모델이 다양한 환경에서 학습할 수 있도록 백색 소음 또는 배경 소음을 추가합니다.

- **적용 방안**:
    - Gaussian Noise, Pink Noise 등 다양한 유형의 소음을 추가.
    - SNR 값을 조정해 소음 비율을 다양하게 적용.

#### **2.2. Pitch Shifting**

오디오의 음조를 변화시켜 모델이 다양한 음역대를 학습하도록 유도합니다.

- **적용 방안**:
    - 오디오의 음조를 ±1~2 반음 조정.
    - 변형된 신호가 원래 시간 길이를 유지하도록 재조정.

#### **2.3. Time Stretching**

시간 스케일을 조정해 신호의 길이를 늘리거나 줄여 시간적 변화를 학습할 수 있도록 합니다.

- **적용 방안**:
    - STFT(Short-Time Fourier Transform)를 사용해 시간 축을 조정.
    - ±10% 이내로 시간 스케일을 변경해 과도한 왜곡 방지.

#### **2.4. SpecAugment**

Mel-spectrogram에서 특정 영역을 마스킹하여 모델이 다양한 변형에 대해 강건해지도록 합니다.

- **적용 방안**:
    - Frequency Masking: 특정 주파수 대역을 무작위로 마스킹.
    - Time Masking: 특정 시간 영역을 무작위로 마스킹.
    - Time Warping: 시간 축을 왜곡하여 변형된 데이터를 생성.

#### **2.5. Room Impulse Response (RIR)**

잔향을 추가해 실제 환경에서의 오디오 데이터를 시뮬레이션.

- **적용 방안**:
    - 다양한 환경(실내, 야외)의 RIR 데이터를 사용하여 잔향 추가.
    - 다양한 반사 특성을 가진 공간 조건을 반영.

#### **2.6. Mixing**

서로 다른 두 오디오 데이터를 혼합하여 새로운 데이터를 생성.

- **적용 방안**:
    - 두 신호를 가중치 비율을 달리하여 혼합.
    - 혼합 후 신호의 RMS 또는 dB를 정규화.

#### **2.7. Volume Perturbation**

볼륨을 무작위로 조정하여 다양한 녹음 환경을 반영합니다.

- **적용 방안**:
    - ±3~5dB 범위에서 볼륨을 무작위로 변경.
    - 조정된 신호의 특징이 손상되지 않도록 범위를 제한.

#### **2.8. 데이터 증강과 Knowledge Distillation 연계**

증강된 데이터를 활용해 Teacher 모델에서 Student 모델로 지식을 전달.

- **적용 방안**:
    - Teacher 모델이 생성한 증강 데이터를 Student 모델에 학습.
    - Teacher/Student 간 차원을 맞추고 Intermediate Representation Distillation 적용.

---

### **3. 경량화 및 증강 기법의 실험 적용**

#### **3.1. 경량화의 타겟 정의**

- **ASR (Automatic Speech Recognition)**: Word Error Rate(WER)를 낮추기 위해 음성 데이터를 효율적으로 압축.
- **AAC (Audio Captioning)**: SPIDEr 점수를 높이기 위해 다양한 데이터 증강을 적용.

#### **3.2. 모델 구조 고려**

- 경량화 모델이 **Teacher 모델**로부터 지식을 전달받을 때, 적합한 구조와 차원 매칭 필요.
- Pruning, Quantization, Knowledge Distillation 등 경량화 방법과 데이터 증강 기법 병행.

# Reference
**데이터셋 별로 “성질이 비슷한 것”끼리 전처리 코드를 묶어서 사용하는 전략**은 실제로 많이 쓰이는 방식입니다.

- **WavCaps, AudioCaps** (오디오 캡셔닝 / 오디오+텍스트)
- **GigaSpeech, LibriSpeech** (ASR 중심, 스피치+정확한 텍스트 트랜스크립션)
- **MusicNet, Clotho** (음악∙환경음 + 여러 라벨 형태, 특히 Clotho는 오디오 캡션이지만 다양한 사운드 이벤트를 포함)

아래는 이러한 분류의 **근거**와, 실제로 **3개 전처리 코드**로 통합할 때 고려하면 좋은 **설계 팁**입니다.

---

# 1. 묶음(그룹)별 특성

## 1) WavCaps, AudioCaps (오디오 캡셔닝)

- **주 목적**: 오디오를 입력으로 “자연어 캡션”을 생성
- **공통점**:
    - 오디오 + 여러 버전(annotators 별) 캡션 텍스트
    - 비교적 짧은 오디오 (수 초 ~ 수십 초)
    - 무음/배경음 자체가 **의미 있는 요소**일 수도 있음
- **전처리 주안점**:
    - **잡음 제거를 과도**하게 하면, 캡션 대상이 될 배경 사운드까지 날아갈 수 있음
    - 텍스트 라벨(캡션)의 **품질 관리** (불필요한 문구 제거, 특수문자 처리 등)
    - 스테레오/샘플레이트 통일, 길이 제한(최대 n초) 등

## 2) GigaSpeech, LibriSpeech (음성인식/ASR)

- **주 목적**: **정확한 음성-텍스트 매핑** (음성인식, 언어모델 학습용)
- **공통점**:
    - 대규모 스피치(“말소리”) 데이터
    - 라벨이 고품질(전사자 주석, 혹은 부분 자동→수정)
    - 녹음 환경∙도메인이 다채롭지만, **주로 “사람 음성”**에 초점
- **전처리 주안점**:
    - **VAD/무음 제거** 또는 세그먼트화(발화 단위)
    - **노이즈/리버브** 제거(ASR 성능 향상 목적)
    - 긴 오디오(특히 GigaSpeech) 분할, 텍스트와 시간동기(Alignment) 체크
    - 라벨(문장)의 구두점 처리, 대소문자 통일 등

## 3) MusicNet, Clotho (음악 / 다양한 사운드)

- **MusicNet**: 클래식 음악 중심, 멀티 트랙 악기 라벨(피아노, 바이올린 등)
- **Clotho**: 환경·음악·인간음 등 **다양한 사운드** + **5개 캡션**
- **공통점**:
    - **음악 및 일반 사운드**가 혼재 (MusicNet은 음악 라벨링, Clotho는 캡셔닝)
    - 상대적으로 **오디오 이벤트(음색, 악기, 효과음) 보존**이 중요
- **전처리 주안점**:
    - **주파수 필터링**(음악에서 특정 대역 강조/분석용),
    - **스테레오 유지**가 필요한 경우(음상(스테레오 이미지) 중요) vs 모노 변환
    - Clotho처럼 **캡셔닝** 태스크이면, 무음·잡음 자체가 중요한 요소일 수도 있어 최소한의 제거
    - MusicNet 악기별 파트/라벨 alignment(미디 파일 연동) 확인

---

# 2. “3개 전처리 코드” 설계 시 고려할 점

1. **코드 재사용성 vs. 커스터마이징**
    
    - 세 그룹마다 “공통 함수”(예: 리샘플, 모노 변환, 정규화)는 비슷하게 재사용
    - 그러나 “VAD/노이즈 제거” 강도, 라벨 전처리 로직 등은 각 그룹별로 달라야 함
2. **구조적인 분리**
    
    - 예:
        - `common_preprocessing.py`: 공통 로직(파일 로드, 채널 변환, 리샘플)
        - `asr_preprocessing.py`: (GigaSpeech, LibriSpeech) 전용 로직(VAD, 텍스트 정상화 등)
        -c, Clotho) 캡션, 텍스트 토큰 처리, 필요 최소 잡음 제거
        - `music_preprocessing.py`: (MusicNet) 악기별 트랙 처리, 스테레오 유지 여부, 주파수 대역 강조
    - 그리고 **3개의 메인 파이프라인**(아래 예시)에서 그룹별 데이터셋에 맞게 가져다 씀.
3. **데이터셋별 config**
    
    - 각 데이터셋마다:
        - `sample_rate`, `max_duration`, `vad_threshold`, “필요한 노이즈 제거 레벨” 등 파라미터를 딕셔너리(config)로 정의해두고,
        - 전처리 함수가 이 config를 받아 작동하도록
    - 이렇게 하면, 3개의 전처리 코드라도 내부 로직을 유연하게 공유 가능
4. **라벨 형태**
    
    - ASR: 전사(문장), 단어 단위 time alignment(있을 수도)
    - 캡션: 캡션 문장(한 오디오에 다수)
    - 음악 라벨: 악기 이벤트(시작 시점~끝 시점) or 벌스/코러스 구간
    - → **라벨을 어떻게 읽고, 저장**할지(“.csv”, “.json” 등) 다르므로, 별도 함수 분리 추천

---

## 예시: 3개 파이프라인 구조

```python
# common_utils.py
def load_audio(file_path, target_sr=16000):
    # ...
    return waveform, sr

def resample_mono(waveform, sr, target_sr=16000):
    # ...
    return new_waveform, target_sr

def normalize_waveform(waveform, max_peak=0.9):
    # ...
    return normalized

# asr_preprocessing.py
from common_utils import load_audio, resample_mono, normalize_waveform

def asr_preprocess(file_path, config):
    waveform, sr = load_audio(file_path, config['sample_rate'])
    # 모노 변환
    waveform, sr = resample_mono(waveform, sr, config['sample_rate'])
    # 무음 제거 (VAD)
    if config['apply_vad']:
        waveform = remove_silence(waveform, sr, threshold=config['vad_threshold'])
    # 노이즈 억제
    if config['denoise']:
        waveform = spectral_subtraction(waveform)
    # 정규화
    waveform = normalize_waveform(waveform, max_peak=0.9)
    # 길이 제한
    waveform = pad_or_trim(waveform, config['max_length'])
    return waveform

# caption_preprocessing.py
from common_utils import load_audio, resample_mono, normalize_waveform

def caption_preprocess(file_path, config):
    waveform, sr = load_audio(file_path, config['sample_rate'])
    waveform, sr = resample_mono(waveform, sr, config['sample_rate'])
    # 오디오 캡셔닝에서는 무음이 이벤트일 수도 → 굳이 VAD로 자르지 않을 수 있음
    # 대신 극단적인 클리핑만 정규화
    waveform = normalize_waveform(waveform, max_peak=0.95)
    # 필요하면 최소 노이즈 제거
    # ...
    # 길이 제한
    waveform = pad_or_trim(waveform, config['max_length'])
    return waveform

# music_preprocessing.py
def music_preprocess(file_path, config):
    waveform, sr = load_audio(file_path, config['sample_rate'])
    # 여기서는 스테레오 유지할 수도
    if config.get('convert_mono', False):
        waveform, sr = resample_mono(waveform, sr, config['sample_rate'])
    # 밴드패스 필터(예: 20~20kHz) -> 크게 의미 없지만, 특정 악기 강조용
    # ...
    # 정규화
    waveform = normalize_waveform(waveform, max_peak=0.9)
    # 길이 제한 or 세그먼트 나누기
    # ...
    return waveform
```

- **주의**: 코드 예시는 간략화된 것이며, 실제로는 더 많은 에러 처리가 필요합니다(파일이 손상되었을 때 예외 처리 등).

---

# 3. 정리 & 결론

- **그룹별로 전처리를 따로 작성**하는 것은 좋은 접근입니다.
- 공통되는 부분(오디오 로딩, 기본 정규화, 채널 변환 등)은 **공용 함수**로 만들어놓고,
- 그룹(ASR vs. 캡셔닝 vs. 음악/이벤트)에 맞춰 차별화된 전처리(예: **VAD 적용 여부, 노이즈 억제 수준**, 라벨 파일 처리 로직 등)를 추가하는 형태가 이상적입니다.

이렇게 하면,

1. **중복 코드**를 줄이고
2. 각 데이터셋(혹은 태스크)의 **특수성**을 잘 반영  
    할 수 있습니다.

따라서, “**WavCaps/AudioCaps_1m**” 전처리 코드, “**GigaSpeech/LibriSpeech**” 전처리 코드, “**MusicNet/Clotho**” 전처리 코드로 **3개**를 나누어서 관리하되,  
**공통 유틸리티**(오디오 로딩, 리샘플, 정규화, 파일 I/O 등)는 모듈화하여 **재사용**하는 방식을 추천합니다.

![](https://i.imgur.com/73oFfkh.png)
토큰 pruning on off