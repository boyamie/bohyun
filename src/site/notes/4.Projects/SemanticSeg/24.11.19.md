---
{"dg-publish":true,"permalink":"/4-projects/semantic-seg/24-11-19/"}
---

| **모델**                               | **강점**                          | **추천 이유**                  |
| ------------------------------------ | ------------------------------- | -------------------------- |
| **Mask2Former**                      | 복잡한 세그멘테이션 작업에서 높은 정확도          | 뼈의 구조와 경계를 정밀하게 구분.        |
| **Swin Transformer + UperNet**       | 고해상도 데이터와 복잡한 클래스 처리에 적합        | 뼈의 세부 구조를 고정밀도로 분리 가능.     |
| **Pyramid Vision Transformer (PVT)** | 메모리 효율적이며 고성능 Transformer 구조 제공 | 고해상도 뼈 데이터 작업에서 효율적.       |
| **MobileViT**                        | 경량화된 모델, 제한된 자원 환경에서 적합         | 리소스 제약이 있는 환경에서 사용.        |
| **EfficientNet**                     | 효율성과 정확성 간의 균형 제공               | 고성능이 필요하지 않은 경량 세그멘테이션 작업. |
## Mask2Former
![](https://i.imgur.com/kSHMMyN.png)

31개 매개변수 확장

매개변수

- [](https://huggingface.co/docs/transformers/model_doc/mask2former#transformers.Mask2FormerConfig.backbone_config)**backbone_config** ( `PretrainedConfig`또는 `dict`, _선택 사항_ , 기본값은 `SwinConfig()`) — 백본 모델의 구성입니다. 설정하지 않으면 에 해당하는 구성이 `swin-base-patch4-window12-384`사용됩니다.
- [](https://huggingface.co/docs/transformers/model_doc/mask2former#transformers.Mask2FormerConfig.backbone)**backbone** ( `str`, _선택 사항_`backbone_config` ) — is 일 때 사용할 백본의 이름입니다 `None`. `use_pretrained_backbone`is 인 경우 `True`timm 또는 transformers 라이브러리에서 해당 사전 학습된 가중치를 로드합니다. `use_pretrained_backbone` is 인 경우 `False`백본의 구성을 로드하고 이를 사용하여 랜덤 가중치로 백본을 초기화합니다.
- [](https://huggingface.co/docs/transformers/model_doc/mask2former#transformers.Mask2FormerConfig.use_pretrained_backbone)**use_pretrained_backbone** ( `bool`, _선택 사항_ , `False`) — 백본에 사전 훈련된 가중치를 사용할지 여부.
- [](https://huggingface.co/docs/transformers/model_doc/mask2former#transformers.Mask2FormerConfig.use_timm_backbone)**use_timm_backbone** ( `bool`, _선택 사항_ , ) — timm 라이브러리에서 `False`로드할지 여부 . 이면 백본은 transformers 라이브러리에서 로드됩니다.`backbone``False`
- [](https://huggingface.co/docs/transformers/model_doc/mask2former#transformers.Mask2FormerConfig.backbone_kwargs)**backbone_kwargs** ( `dict`, _선택 사항_ ) — 체크포인트에서 로드할 때 AutoBackbone에 전달되는 키워드 인수 예 : . 설정된 `{'out_indices': (0, 1, 2, 3)}`경우 지정할 수 없습니다 .`backbone_config`
- [](https://huggingface.co/docs/transformers/model_doc/mask2former#transformers.Mask2FormerConfig.feature_size)**feature_size** ( `int`, _선택 사항_ , 기본값은 256) — 결과 피처 맵의 피처(채널).
- [](https://huggingface.co/docs/transformers/model_doc/mask2former#transformers.Mask2FormerConfig.mask_feature_size)**mask_feature_size** ( `int`, _선택 사항_ , 기본값은 256) — 마스크의 피처 크기. 이 값은 피처 피라미드 네트워크 피처의 크기를 지정하는 데에도 사용됩니다.
- [](https://huggingface.co/docs/transformers/model_doc/mask2former#transformers.Mask2FormerConfig.hidden_dim)**hidden_dim** ( `int`, _선택 사항_ , 기본값은 256) — 인코더 레이어의 차원.
- [](https://huggingface.co/docs/transformers/model_doc/mask2former#transformers.Mask2FormerConfig.encoder_feedforward_dim)**인코더_피드포워드_dim** ( `int`, _선택 사항_ , 기본값은 1024) — 픽셀 디코더의 일부로 사용되는 변형 가능한 디트롤 인코더의 피드포워드 네트워크 차원입니다.
- [](https://huggingface.co/docs/transformers/model_doc/mask2former#transformers.Mask2FormerConfig.encoder_layers)**인코더_레이어** ( `int`, _선택 사항_ , 기본값 6) — 픽셀 디코더의 일부로 사용되는 변형 가능한 detr 인코더의 레이어 수입니다.
- [](https://huggingface.co/docs/transformers/model_doc/mask2former#transformers.Mask2FormerConfig.decoder_layers)**decoder_layers** ( `int`, _선택 사항_ , 기본값은 10) — Transformer 디코더의 레이어 수.
- [](https://huggingface.co/docs/transformers/model_doc/mask2former#transformers.Mask2FormerConfig.num_attention_heads)**num_attention_heads** ( `int`, _선택 사항_ , 기본값은 8) — 각 어텐션 레이어의 어텐션 헤드 수.
- [](https://huggingface.co/docs/transformers/model_doc/mask2former#transformers.Mask2FormerConfig.dropout)**dropout** ( `float`, _선택 사항_ , 기본값은 0.1) — 임베딩의 모든 완전 연결 계층에 대한 드롭아웃 확률, 인코더.
- [](https://huggingface.co/docs/transformers/model_doc/mask2former#transformers.Mask2FormerConfig.dim_feedforward)**dim_feedforward** ( `int`, _선택 사항_ , 기본값은 2048) — 변압기 디코더의 피드포워드 네트워크의 피처 차원.
- [](https://huggingface.co/docs/transformers/model_doc/mask2former#transformers.Mask2FormerConfig.pre_norm)**pre_norm** ( `bool`, _선택 사항_ , 기본값 `False`) — 변압기 디코더에 pre-LayerNorm을 사용할지 여부입니다.
- [](https://huggingface.co/docs/transformers/model_doc/mask2former#transformers.Mask2FormerConfig.enforce_input_projection)**enforcement_input_projection** ( `bool`, _선택 사항_ , 기본값 `False`) — Transformer 디코더에서 입력 채널과 숨겨진 dim이 동일하더라도 입력 투영 1x1 합성곱을 추가할지 여부입니다.
- [](https://huggingface.co/docs/transformers/model_doc/mask2former#transformers.Mask2FormerConfig.common_stride)**common_stride** ( `int`, _선택 사항_ , 기본값은 4) — 픽셀 디코더의 일부로 사용되는 FPN 레벨 수를 결정하는 데 사용되는 매개변수입니다.
- [](https://huggingface.co/docs/transformers/model_doc/mask2former#transformers.Mask2FormerConfig.ignore_value)**ignore_value** ( `int`, _선택 사항_ , 기본값은 255) — 학습 중 무시할 카테고리 ID입니다.
- [](https://huggingface.co/docs/transformers/model_doc/mask2former#transformers.Mask2FormerConfig.num_queries)**num_queries** ( `int`, _선택 사항_ , 기본값은 100) — 디코더에 대한 쿼리 수.
- [](https://huggingface.co/docs/transformers/model_doc/mask2former#transformers.Mask2FormerConfig.no_object_weight)**no_object_weight** ( `int`, _선택 사항_ , 기본값은 0.1) — null(객체 없음) 클래스에 적용할 가중치입니다.
- [](https://huggingface.co/docs/transformers/model_doc/mask2former#transformers.Mask2FormerConfig.class_weight)**class_weight** ( `int`, _선택 사항_ , 기본값은 2.0) — 교차 엔트로피 손실에 대한 가중치입니다.
- [](https://huggingface.co/docs/transformers/model_doc/mask2former#transformers.Mask2FormerConfig.mask_weight)**mask_weight** ( `int`, _선택 사항_ , 기본값은 5.0) — 마스크 손실에 대한 가중치입니다.
- [](https://huggingface.co/docs/transformers/model_doc/mask2former#transformers.Mask2FormerConfig.dice_weight)**dice_weight** ( `int`, _선택 사항_ , 기본값은 5.0) — 주사위 손실의 가중치입니다.
- [](https://huggingface.co/docs/transformers/model_doc/mask2former#transformers.Mask2FormerConfig.train_num_points)**train_num_points** ( `str`또는 _선택 사항_`function` , 기본값은 12544) — 손실 계산 중 샘플링에 사용되는 포인트 수.
- [](https://huggingface.co/docs/transformers/model_doc/mask2former#transformers.Mask2FormerConfig.oversample_ratio)**oversample_ratio** ( `float`, _선택 사항_ , 기본값은 3.0) — 샘플링된 포인트 수를 계산하는 데 사용되는 오버샘플링 매개변수
- [](https://huggingface.co/docs/transformers/model_doc/mask2former#transformers.Mask2FormerConfig.importance_sample_ratio)**importance_sample_ratio** ( `float`, _선택 사항_ , 기본값은 0.75) — 중요도 샘플링을 통해 샘플링된 포인트의 비율.
- [](https://huggingface.co/docs/transformers/model_doc/mask2former#transformers.Mask2FormerConfig.init_std)**init_std** ( `float`, _선택 사항_ , 기본값은 0.02) — 모든 가중치 행렬을 초기화하기 위한 truncated_normal_initializer의 표준 편차입니다.
- [](https://huggingface.co/docs/transformers/model_doc/mask2former#transformers.Mask2FormerConfig.init_xavier_std)**init_xavier_std** ( `float`, _선택 사항_ , 기본값은 1.0) — HM Attention 맵 모듈의 Xavier 초기화 이득에 사용되는 스케일링 요소입니다.
- [](https://huggingface.co/docs/transformers/model_doc/mask2former#transformers.Mask2FormerConfig.use_auxiliary_loss)**use_auxiliary_loss** ( `boolean``, *optional*, defaults to` True `) -- If` True Mask2FormerForUniversalSegmentationOutput`에는 각 디코더 단계의 로짓을 사용하여 계산된 보조 손실이 포함됩니다.
- [](https://huggingface.co/docs/transformers/model_doc/mask2former#transformers.Mask2FormerConfig.feature_strides)**feature_strides** ( `List[int]`, _선택 사항_ , 기본값 `[4, 8, 16, 32]`) — 백본 네트워크에서 생성된 기능에 해당하는 기능 스트라이드입니다.
- [](https://huggingface.co/docs/transformers/model_doc/mask2former#transformers.Mask2FormerConfig.output_auxiliary_logits)**output_auxiliary_logits** ( `bool`, _선택 사항_ ) — 모델이 이를 출력해야 하는지 `auxiliary_logits`여부입니다.

[Mask2FormerModel](https://huggingface.co/docs/transformers/v4.46.3/en/model_doc/mask2former#transformers.Mask2FormerModel) 의 구성을 저장하는 구성 클래스입니다 . 지정된 인수에 따라 Mask2Former 모델을 인스턴스화하고 모델 아키텍처를 정의하는 데 사용됩니다. 기본값으로 구성을 인스턴스화하면 Mask2Former [facebook/mask2former-swin-small-coco-instance](https://huggingface.co/facebook/mask2former-swin-small-coco-instance) 아키텍처와 유사한 구성이 생성됩니다.

구성 객체는 [PretrainedConfig](https://huggingface.co/docs/transformers/v4.46.3/en/main_classes/configuration#transformers.PretrainedConfig) 에서 상속되며 모델 출력을 제어하는 ​​데 사용할 수 있습니다. 자세한 내용은 [PretrainedConfig 의 설명서를 읽어보세요.](https://huggingface.co/docs/transformers/v4.46.3/en/main_classes/configuration#transformers.PretrainedConfig)

현재 Mask2Former는 [Swin Transformer](https://huggingface.co/docs/transformers/model_doc/swin) 만을 백본으로 지원합니다.


![](https://i.imgur.com/xIPWypv.png)


```bash
root@instance-13781:~/bohyun/ultralytics# python data_utils.py 
Starting convert data to coco format
800it [00:05, 139.29it/s]
Finish convert data to coco format
Starting convert yolo to coco format
800it [00:10, 73.06it/s]
Finish convert yolo to coco format
Train images:  640
Train labels:  640
Valid images:  160
Valid labels:  160
```