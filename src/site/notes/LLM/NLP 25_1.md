---
{"dg-publish":true,"permalink":"/llm/nlp-25-1/"}
---

중간 40% + 숙제 30%(프로그래밍 과제 5개 정도) + 논문 발표 20% + 태도 10%
중간고사 5월 15일

## 0313
![](https://i.imgur.com/VOb66GW.png)
스팸메일 분류: 1 | 0 -> binary classification

input text를 vector로 만들기 위해 encoder로 feature를 뽑아낸다.
그 feature를 사용해서 predictive model은 classifier가 되어서 예측을 한다.

![](https://i.imgur.com/AYDnbSg.png)
unique한 단어들의 set을 voca라고 하고 v로 표시한다.
bag-of-words는 단어의 모음이다.
문장을 성공적으로 벡터로 만들어내면 많은 정보들이 날라가겠지만 컴퓨터에 인식시킬 순 있다.

첫번째 단점은 너무 길다는 것이다.
![](https://i.imgur.com/C9kRvsC.png)
단어, 문장 표현하는데 수천만 길이를 써야하고 그 중 대부분은 0이다.
이게 얼마나 공간 낭비인가?
이걸 sparse하다고 한다. 희소행렬 등..
sparse한 matrix는 공간낭비 시간낭비로 이어진다.
즉 벡터를 갖다가 nn에 태울건데 모든 정보 간의 관계를 학습시키기 때문에 n^2 이상의 파라미터가 필요하다.

두번째 단점은 순서가 중요한데 그 정보가 날라간다.
![](https://i.imgur.com/j3KqdU8.png)
감정 분류 -> 네개의 포커스가 있다고 쳐보자 (실제론 수십 수백만개가 있음)

frequency로 mapping
![](https://i.imgur.com/zEW4sZC.png)

전처리 의미있는 단어만 남기기
![](https://i.imgur.com/72MMf3y.png)
stop words: 구조를 위한 단어
![](https://i.imgur.com/KJj7mjz.png)
줄기만 남기자.
가지를 다 쳐냄
tune을 tuning tuned tunes를 다 voca에 따로 보관하면 문제가 있다.
그래서 ing, ed, es를 다 잘라버린다.
ed로 끝나면 없애 이런식으로 작동한다.
![](https://i.imgur.com/jngpetV.png)
feature가 결과를 학습하는데 얼마나 중요한지..
w_f 가 모델 파라미터에 속하는 것
학습이 되는 모델 파라미터
파라미터기 때문에 theta라고도 쓴다.
![](https://i.imgur.com/kn9nBjq.png)
전체적인 점수에 경향성 더하기
![](https://i.imgur.com/7p7tbzu.png)
logistic regression: 머신러닝적으로 접근
naiive bayes: 확률적으로 접근
![](https://i.imgur.com/9taPPwF.png)
positive class에 속할 probability로 시그모이드 function을 정의하고 사용해서 정수를 확률화하기 위해 사용한다.
![](https://i.imgur.com/o2Yo0dk.png)
pos , neg 간의 ratio
pos일 확률을 p라고 하면 neg일 확률은 1-p이다.
그러면 odds ration는 
![](https://i.imgur.com/IL6OLCN.png)
odds에 log를 씌우면 
![](https://i.imgur.com/piqcHDQ.png)
probabilty를 실수범위(−∞, ∞)로 바꿀 수 있다.
역함수를 취하면 실수 범위를 probability로 바꿀 수 있다.
logit을 linear regression model로 사용할 수 있다. z(−∞, ∞)로 사용할 수 있게 된다.

0.5를 decision boundary라고 한다.
![](https://i.imgur.com/mH2ldBe.png)
미분이 굉장히 쉽다.
![](https://i.imgur.com/4NF8jZE.png)

![](https://i.imgur.com/c7oRajs.png)
확률이 0이 안되게 만들어줌

prior와 반대되는 개념인 posterior
![](https://i.imgur.com/LP6GNIB.png)
사후확률 (등장하고 나서야 알 수 있다.)
likelihood에 prior를 줘서 클래스간 imbalance한 상태까지도 고려한다.
![](https://i.imgur.com/ESeHEwG.png)
w1x1 + w2x2..
에서 x가 중요했었다
근데 이게 2010 대가 오면서 GPU가 발전돼서 병렬적인 계산의 효율성이 증가하고 nn 모델로 계산가능하게 되었다.
![](https://i.imgur.com/179zCxp.png)
논리회로 연산자 중 하나인 XOR.
![](https://i.imgur.com/KQ7nUo9.png)
여러 feature가 들어갈 때 w를 곱해서 더함
![](https://i.imgur.com/YSp7Y7N.png)
이걸 사용해서 xor모델을 구현할 수 있을까?
못나눔
![](https://i.imgur.com/ov4mFED.png)
decision boundary
xor을 나누기 위해선 선이 line이면 안된다는 결론이 생긴다.
![](https://i.imgur.com/iz3QH1C.png)
여러 레이어에 퍼셉트론을 태우자
퍼셉트론을 여러 겹 태운다는 의미
feedforward nn이라고 하고 mlp라고한다.
![](https://i.imgur.com/7MLpElD.png)
병렬적으로 matrix multiplycation을 GPU쓰면 성능을 끌어올릴 수 있음
![](https://i.imgur.com/udkAmiE.png)
![](https://i.imgur.com/xblM8gI.png)
파란 선이 시그모이드로 0부터 1사이의 값을 가짐.
시그모이드가 0~1이 되는게 불만이라서 만든게 주황색인
하이퍼볼릭 탄젠트 (중간 값이 0이 됨)
요즘은 relu를 씀 가장 중요한건 gradient vanishing을 없애준다는 것이다.

## Evaluation
![](https://i.imgur.com/uLbOooo.png)
classification evaluation은 데이터에 대해서 이야기를 안해볼 수 없는데 데이터셋이라는 거에 종류가 많이 있다.
원하는건 데이터 사용해서 학습시키고 unseen data를 평가한다
근데 unseen이니까 볼 수 없어서
테스트데이터라고 하는것은 unseen데이터를 한거라서 학습과정에 절대로 들어가선 안된다.

loss function에 regulazation을 해준다. w가 너무 커지지 않도록 제약을 걸어준다.
정해야 하는 것은 람다인데 학습을 할 수는 없다.
![](https://i.imgur.com/t3fwYUo.png)
다섯개 다 학습시키고 고른다. (그리드 서치)
0.01이 최고라고 해보자.
best of what?
가지고 있는 것은 두 개의 데이터셋이다.
test data로 고르겠다.. 근데 쓰여선 안된다.

test data은 unseen data를 simulation한다.
![](https://i.imgur.com/q0wp5Yd.png)
온 세상 데이터를 네모라고 하면 네모 데이터를 대표하는 애들이 test dataset이다.
람다를 정할 때 test데이터에 맞춰버리면 test data에 overfit한 데이터가 나와버린다.
그래서 한번 더 잘라버리는데 이걸 val 데이터라고 하고, 테스트 데이터를 모방하는 unseen data라고 할 수 있다.
![](https://i.imgur.com/bpHc8d9.png)
람다와 같은 것들을 하이퍼파라미터를 통해 선택한다.
6:2:2
7:1.5:1.5 정도로 나누는 것이 불문율이라고 한다.
hyperparameter는 인간이 정해줘야 하는 것이다.
![](https://i.imgur.com/G18CKnw.png)
하이퍼 파라미터가 될 수 있는 것들

맞았다 틀렸다가 아니라 다른걸 써야한다.
틀렸다의 정의 방법엔 두가지가 있다.
![](https://i.imgur.com/u2isUDM.png)
recall, precision 둘 다 높아야 높게 나옴
![](https://i.imgur.com/skaXI3J.png)
distance matrix
## Minimum Edit Distance

# 250417 Neural Language Model

![](https://i.imgur.com/HY7LuND.png)
softmax 취해서 단어 갯수만큼 간다.
전체 단어 voca만큼
전체 단어 각각에 대해 확률 예측
![](https://i.imgur.com/E8S1lFp.png)
예측한 클래스는 voca size만큼
concat: 벡터를 붙여서 하나의 벡터로 만드는 연산
![](https://i.imgur.com/2mLYdSa.png)
voca size만큼의 output을 내놓아야한다.
![](https://i.imgur.com/MIqB1fu.png)
f는 activation function이 있어서 골라쓰면 된다.

n-gram language 모델보다 좋은점
![](https://i.imgur.com/RGiy8hc.png)
단어갯수의 4승 만큼의 레이어가 필요했지만 n-gram보다 크게 줄어든다는 장점이 있다.
sparsity issue가 어느정도 해결된다
정보의 공유가 일어나서 임베딩의 유지가 일어난다.

![](https://i.imgur.com/wnFeWgu.png)
input size를 무조건 늘리면 expenentially 늘어나기 때문에 이게 해결책은 아니다. 그렇기 때문에 멀리 떨어져있는 context를 반영할 수 없다.
![](https://i.imgur.com/j8oNFjK.png)
long context 학습이 구조적으로 가능해보이지만 안된다.
![](https://i.imgur.com/58nC9Qw.png)
만개 정도의 프리퀀트 단어들만 고려

![](https://i.imgur.com/mU6fzbh.png)
그럼 많이 나오는 단어인지 아니냐만 예측하더라도
수많은 덜나오는 단어 계산 안해도 된다는 개념

그럼 모든 단어에 대한 점수 예측 안해도 됨

![](https://i.imgur.com/UwhPnES.png)
두번째는 negative sampling구해서 pos,neg만 학습을 하자.

ppl: perplexity
![](https://i.imgur.com/J1r02UF.png)
x:input feature, y: output label

L2, L1 차이
![](https://i.imgur.com/G9dQEuo.png)
	L2는 파라미터 하나가 커지는걸 방지하는 효과가 있다
	파라미터에 가해지는 힘
![](https://i.imgur.com/C1nz4Oj.png)
L2는 큰걸 먼저 줄이는데 의의가 있는거고

![](https://i.imgur.com/wCBqzBS.png)
제일 줄이기 쉬웠던걸 0으로 줄이는 효과
따라서 L1은 parameter reduction
어떤 feature가 더 중요한지 모르기때문에 weight가 0이 되는 feature는 덜 중요하다는 방식으로..

![](https://i.imgur.com/07nRKzD.png)
test 때는 dropout 안쓰도록 한다. (훈련떄만 씀)
dropout은 앙상블 효과도 있다는 연구 결과도 있다.

LSTM은 RNN에서 vanishing gradient 문제를 해결하기 위해 나온 모델이다.
![](https://i.imgur.com/t1PwG4G.png)
context vector를 보완하기 위해
c, cell state는 long-term information을 가지고 다니는 애인데
input gate는 새로운 정보를 얼마나 가지고 다닐거냐
output gate는 next hiddenstate에 c를 얼마나 반영할거냐
forget gate는 c를 얼마나 잊어버릴거냐를 결정
![](https://i.imgur.com/gfz2nUd.png)
c_m+1 셀스테이트 얼마나 ..
cm 어느정도 잊어버리고
새로들어오는정보(~틸다)는 inputgate만큼 반영시킬거다.
c틸다는 새로들어온

하다마드프로덕트 엘리먼트끼리 하는 연산자

시험: 구조 다 외울필요 없음
*LSTM이 어떻게 연산하고 어떤점이 뛰어나며* 

2014년 GRU나옴 한국인 조경현교수님이 개발
![](https://i.imgur.com/aBgMfuZ.png)
z, 1-z 쓰면 똑같은거 아니냐 -> z라는 업데이트 게이트 만들어서 input, forget 동시에 수행하는 하나의 게이트

![](https://i.imgur.com/UkURwXK.png)
Elman network는 gradient vanishing 때문에 멀리 떨어진 관계간의 구조적 문제는 해결할 수 없다.

Bidirectional: 반대로 RNN 돌리기
![](https://i.imgur.com/VOIRCqd.png)
왼쪽 오른쪽 둘 다 쓰는게 제일 좋을 것

![](https://i.imgur.com/vF2EkY0.png)
각각 representation concat해서 씀

![](https://i.imgur.com/BOal782.png)
play는 다의어(polysemy)
세개 의미를 다 내포하는 하나의 벡터를 만들긴 어려워.
인간은 문맥으로 이해를 한다.

![](https://i.imgur.com/dOXLBqy.png)
고정된 임베딩이 아니라 문맥에서 나오는 임베딩을 얻고싶다.

ELMo -> Bert 
![](https://i.imgur.com/xKNptkP.png)
워드임베딩은 그대로 두고 감마는 우리가 하고자하는 테스크에 파인튜닝시켜서 쓰면됨

![](https://i.imgur.com/u7o6bdv.png)
레이블 없는 데이터 가져다가 레이블 만드는 방법

transfer learning(fine-tuning)
![](https://i.imgur.com/IyMJial.png)
파인튜닝 없이 학습한거 만으로도 기존 학습 이김

# 0424 Sequence Labeling
NER: 사람이름, 장소이름 식별하는 task
![](https://i.imgur.com/kLjvK3P.png)
트리 형식으로 
![](https://i.imgur.com/05qhKYK.png)
어쿠스틱  -> 음성신호가 어떤 단어를 의미하는지

POS tagging, NER 요즘 잘 안함
![](https://i.imgur.com/Ecsuh95.png)
TTS: 문장 읽어주는
![](https://i.imgur.com/s7Wk1CY.png)
단어를 일반형으로 만들어주기
렘마타이제이션

![](https://i.imgur.com/e01I3w2.png)
observed를 보고 prob model을 거꾸로 추론하는걸 generative sequence라고 함.
p(y|x)에서 
![](https://i.imgur.com/DheXeCQ.png)
hidden state x가 주어졌을 때 y(단어)를..

![](https://i.imgur.com/EgbMqDb.png)
주어 동사 목적어 (관측한) 시퀀스를 토대로 i throw a ball을 생성 (POS)

![](https://i.imgur.com/tDvbODM.png)
우리가 POS 태그라는걸 가정해서 생성해냈다고 가정하지만 품사의 sequence는 못봤다. 볼 수 없는 정보임
x2를 생성하는데 있어서 세타1은 영향을 안준다는 것(markov 가정: 현재 상태에만 영향을 받는다는 가정)
hidden state가 있는 markov가정이 있는 모델이라는 뜻
![](https://i.imgur.com/jx1KZcm.png)
두 경우 다 확률이 있을텐데 주 동 목이 제일 높을거임
HMM은 통계적인 정보를 씀
![](https://i.imgur.com/ZST7Qld.png)
다 모으면 100퍼센트가 됨

처음에 왜 주어가 나올지를 정의하는걸 파이라고 함
![](https://i.imgur.com/RAczYfa.png)

![](https://i.imgur.com/1rGmHR4.png)
직전꺼만 보고 나머지는 보지 않겠다는게 markov assumption

pos tag에서 태그들이 아웃풋을 생성해냈다는 의미
![](https://i.imgur.com/zFU2sn1.png)
주어 동사 목적어는 parsing에 가깝고 정관사 명사 이런것들이 pos에 가깝다

phoonemes:음소 (사람이들었을때소리로인식하는최소의단위)
![](https://i.imgur.com/wuYhndt.png)
음소가 아니라 주파수만 듣는데 sip, zip 결정해야되는데 예시에서 보는거처럼[]이거만 알고있는

![](https://i.imgur.com/r2Z0Slb.png)
A transition probability가 존재
![](https://i.imgur.com/Ibb9Wal.png)
데이터가 주어졌을 때 얼마나 잘 설명하는지 모델을 평가
![](https://i.imgur.com/ebnecCs.png)
A tansmition B emition 구하는 과정이 learning이다.
EM 알고리즘을 사용하는 기법 하나 고정하고 하나구하고
그걸 번갈아가면서 수렴할 때까지 한다

![](https://i.imgur.com/FrlacVh.png)
T개의 state는 Q에서 나옴 명사든 동사든..

![](https://i.imgur.com/3fjzDtX.png)
확률을 최고로 만드는 세타 셋을 만들어야하는데 P
![](https://i.imgur.com/aI98o2F.png)
??

![](https://i.imgur.com/lp2MubT.png)
최고의 pos tagging을 찾는 데 있어서 
![](https://i.imgur.com/LIWFtmq.png)
우리가 찾고자하는 pos태그다.
만약 문장의 길이가 
![](https://i.imgur.com/KyZsPQy.png)
제일 좋은거 하나 고르기
![](https://i.imgur.com/qhZ5axe.png)
직전 상태
![](https://i.imgur.com/bUKb4j4.png)
세로 축은 태그이다.
가로 축은 sequence이다
![](https://i.imgur.com/zjlezmu.png)
두번째까지 최적의 경로를 구한 것이다. 중간에 선택되지 않는 경로는 다 빼버리고 다 고려해서 최고로나온 단 하나의 경로 하나를 고르면 우리가 원하는 pos tag의 경로가 된다.
![](https://i.imgur.com/sw4xGhs.png)
가장 높은 pos probability가 된다
![](https://i.imgur.com/yYN8j2p.png)
각각 step, sequence에서 가장 높은 비터비(경로)를 구한다
![](https://i.imgur.com/3vmUnTt.png)

![](https://i.imgur.com/ubN7BMX.png)
initial probability
![](https://i.imgur.com/CUWmijd.png)
아까보다 줄어든 time complexity
![](https://i.imgur.com/tDPGd41.png)
최고를 다 유지하지 말고 일단 지금 봤을 때 NN 확률이 낮기 때문에 빼자!
각 단계 최고 확률이 나오는 k개만 유지하자는게 beam search 개념
greedy algorithm: 최고 1개만 보니까 성능이 안나옴
beam search: top-k를 봄
효율성을 위해 존재하는거고 k를 늘리면 성능은 올라가지만 시간복잡도도 늘어남
![](https://i.imgur.com/mkWbev6.png)
sequence의 레이블을 학습해버리는

![](https://i.imgur.com/9b7eqyY.png)
동사, 명사일 확률이 생김

![](https://i.imgur.com/nc1Kzqi.png)
이 때 P는 로지스틱 회귀를 사용한다고 했으니까 각각의 feature에 대한 세타(weight vector)를..
feature에 대한 weight vector를 학습한다
![](https://i.imgur.com/mRkmTHD.png)
여기선 man이 동사로 쓰였는데
the boat를 봤을 때 앞의 걸(man이 동사라는걸)바꿀 생각을 안한다.
![](https://i.imgur.com/HEvnPtS.png)
직전관계 고려하는 것들의 공통적 문제
이 문제를 해결하기위해 globally normalize model을 사용해야함
![](https://i.imgur.com/p74NzET.png)
엘모같은거 써서 워드임베딩 만들 수 있음
![](https://i.imgur.com/uThkEOb.png)
워드임베딩에 bidirectionalRNN태워서 양방향 만들 수 있고
pos tag에 어떤게 가장 높을지 예측할 수 있을 것이다.
![](https://i.imgur.com/P17WP0R.png)
feature딴에서 반영이 된다고 볼 수 있다
오히려 hmm보다 표현력 풍부한 모델이 될 수 있다
![](https://i.imgur.com/93uTXRL.png)
max pooling인데 nlp다 보니 time에 대한 max를 하나 고르겠다는 의미
![](https://i.imgur.com/ISsYmxt.png)
두 단어의 관계.. 패턴을
그 와중에 max를 고른건 이 sequence에서 제일 센거 하나를 고른다는 ! max over time을 사용해서

![](https://i.imgur.com/Th8mudN.png)
커널사이즈가 3이면
컨켓해서 3개가 하나가 되고..

![](https://i.imgur.com/HAS4tyD.png)
local pattern이 하나의 feature가 됨

![](https://i.imgur.com/MspmGs7.png)
4번째는 7개의 단어 문맥 정보를 갖고있다.
어떤 문제가 발생할까?
4레이어를 쌓았는데 7개 밖에 못봤다는 문제가 생긴다.
![](https://i.imgur.com/Sih0l1J.png)
문맥을 너무 적게 보고있다.(context 수가 layer에 linear하게 늘어난다는 문제)
문제 해결을 위해 cnn을 늘린다
![](https://i.imgur.com/KhhCPJ8.png)
3번째를 만들 때 두 칸씩 띄워서 봄
![](https://i.imgur.com/nHkG1pp.png)
그 다음엔 네칸씩 띄어서 봄
![](https://i.imgur.com/uHVjtzA.png)
exp하게 늘어나서 넓게 볼 수 있다.

![](https://i.imgur.com/n44bW4a.png)
LSTM은 하나씩 집어넣는데 CNN은 한번에 다 집어넣고 병렬적으로 계산할 수 있음.
더 빨라서 cnn을 씀

![](https://i.imgur.com/R091yic.png)
지금까지 word embedding을 봤는데 

![](https://i.imgur.com/ssscHM3.png)
캐릭터로 분해해서 임베딩하면 unknown word가 사라짐
![](https://i.imgur.com/N03yL13.png)
morphologi.. 단어를 형태소의 결합으로 보는 언어들

![](https://i.imgur.com/XjN19lt.png)
분해해서 다 뜻을 가짐

# Attention and Transformers
문장은 순서가 있는 단어의 sequence.
![](https://i.imgur.com/xOefhT6.png)
시퀀스 모델링을 어떻게할지 분류
![](https://i.imgur.com/xKoBfbg.png)
확률에 condition이 없음
문장 X가 들어왔을 때, 하나의 input만 가지고 확률 예측

![](https://i.imgur.com/vqAswam.png)
input을 조건으로 했을 때 output sentence의 확률
이런 것들을 conditioned prediction이라고 부른다.

conditioned prediction 중에 하나
#### Autoregressive
x가 주어졌을 때, y의 확률: y는 sentence
![](https://i.imgur.com/tSZ73gG.png)
한번에 y를 예측하는게 아니라서 시간이 좀 걸린다.
현재시점에서 output을 예측하는 데 있어 직전의 Output을 사용
![](https://i.imgur.com/fL0irxY.png)
seq2seq모델

그럼 autoregressive 하지 않은건 output을 input으로 넣는 과정이 없다.

![](https://i.imgur.com/abcQopv.png)
am을 예측할 땐 인코더에서 나온 정보와 디코더에서 나온 정보를 가지고 있어야 한다.

![](https://i.imgur.com/8U2y3iJ.png)
디코더에선 그동안 생성해낸 결과를 토대로 생성해낸다.

![](https://i.imgur.com/YtFxwqT.png)
end of sentence 토큰 나올때까지 생성
encoderRNN은 넣어서 빚어내고
decoder는 하나씩하나씩 생성해내는데 예측된 단어도 인풋으로 사용한다.
중간중간 나오는 context 벡터를 통해서 예측한다.
![](https://i.imgur.com/6TsTWdd.png)
시퀀스 안의 모든 정보를 다 갖다 씀
weighted average를 위해서 weight를 만들어야한다. 이것이 attention score

token을 벡터로 만들고자하는 기법이다.
attention의 특성은 중요한 context를 요약하는 과정을 거친다.
![](https://i.imgur.com/RODpJJy.png)
중요도도 스스로 학습을 하면
w2도 자동으로 커지도록 학습이 된다.
왜냐하면 모델이 w2커지는게 loss 줄게된다는걸 모델이 학습하기 떄문이다.

단어에 focus.. attend한다. 내가 원하는 단어에 attention을 준다.
![](https://i.imgur.com/5qnfrjr.png)
key: query랑 비교해서 importance를 학습하는 대상
![](https://i.imgur.com/NyJBG1z.png)
유사도, 중요도, 관계도를 만들어 내야 한다.
![](https://i.imgur.com/oxmJohh.png)
잘 학습했다고 가정했을 때, 예시 라는 단어가 attention score가 높을 것이다.
query와 key간의 score를 구했을 때 ..

key와 value는 일반적으로 같음
![](https://i.imgur.com/1PM6hd6.png)
이 결과는 예시라는 정보가 많이 반영된 벡터일 것이기 떄문에,
결과를 generate 했을 때, example이 나올 것

token을 벡터로 인코드했다고 볼 수 있다.
![](https://i.imgur.com/LZ3t2iK.png)

![](https://i.imgur.com/r9us2lD.png)
쿼리, key가 다른 정보를 가지고 있어서 cross라는 단어를 쓴다.
주로 머신 번역에 자주 쓰인다.