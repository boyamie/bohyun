---
{"dg-publish":true,"permalink":"/llm/nlp-25-1/"}
---

중간 40% + 숙제 30%(프로그래밍 과제 5개 정도) + 논문 발표 20% + 태도 10%
중간고사 5월 15일

## 0313
![](https://i.imgur.com/VOb66GW.png)
스팸메일 분류: 1 | 0 -> binary classification

input text를 vector로 만들기 위해 encoder로 feature를 뽑아낸다.
그 feature를 사용해서 predictive model은 classifier가 되어서 예측을 한다.

![](https://i.imgur.com/AYDnbSg.png)
unique한 단어들의 set을 voca라고 하고 v로 표시한다.
bag-of-words는 단어의 모음이다.
문장을 성공적으로 벡터로 만들어내면 많은 정보들이 날라가겠지만 컴퓨터에 인식시킬 순 있다.

첫번째 단점은 너무 길다는 것이다.
![](https://i.imgur.com/C9kRvsC.png)
단어, 문장 표현하는데 수천만 길이를 써야하고 그 중 대부분은 0이다.
이게 얼마나 공간 낭비인가?
이걸 sparse하다고 한다. 희소행렬 등..
sparse한 matrix는 공간낭비 시간낭비로 이어진다.
즉 벡터를 갖다가 nn에 태울건데 모든 정보 간의 관계를 학습시키기 때문에 n^2 이상의 파라미터가 필요하다.

두번째 단점은 순서가 중요한데 그 정보가 날라간다.
![](https://i.imgur.com/j3KqdU8.png)
감정 분류 -> 네개의 포커스가 있다고 쳐보자 (실제론 수십 수백만개가 있음)

frequency로 mapping
![](https://i.imgur.com/zEW4sZC.png)

전처리 의미있는 단어만 남기기
![](https://i.imgur.com/72MMf3y.png)
stop words: 구조를 위한 단어
![](https://i.imgur.com/KJj7mjz.png)
줄기만 남기자.
가지를 다 쳐냄
tune을 tuning tuned tunes를 다 voca에 따로 보관하면 문제가 있다.
그래서 ing, ed, es를 다 잘라버린다.
ed로 끝나면 없애 이런식으로 작동한다.
![](https://i.imgur.com/jngpetV.png)
feature가 결과를 학습하는데 얼마나 중요한지..
w_f 가 모델 파라미터에 속하는 것
학습이 되는 모델 파라미터
파라미터기 때문에 theta라고도 쓴다.
![](https://i.imgur.com/kn9nBjq.png)
전체적인 점수에 경향성 더하기
![](https://i.imgur.com/7p7tbzu.png)
logistic regression: 머신러닝적으로 접근
naiive bayes: 확률적으로 접근
![](https://i.imgur.com/9taPPwF.png)
positive class에 속할 probability로 시그모이드 function을 정의하고 사용해서 정수를 확률화하기 위해 사용한다.
![](https://i.imgur.com/o2Yo0dk.png)
pos , neg 간의 ratio
pos일 확률을 p라고 하면 neg일 확률은 1-p이다.
그러면 odds ration는 
![](https://i.imgur.com/IL6OLCN.png)
odds에 log를 씌우면 
![](https://i.imgur.com/piqcHDQ.png)
probabilty를 실수범위(−∞, ∞)로 바꿀 수 있다.
역함수를 취하면 실수 범위를 probability로 바꿀 수 있다.
logit을 linear regression model로 사용할 수 있다. z(−∞, ∞)로 사용할 수 있게 된다.

0.5를 decision boundary라고 한다.
![](https://i.imgur.com/mH2ldBe.png)
미분이 굉장히 쉽다.
![](https://i.imgur.com/4NF8jZE.png)

![](https://i.imgur.com/c7oRajs.png)
확률이 0이 안되게 만들어줌

prior와 반대되는 개념인 posterior
![](https://i.imgur.com/LP6GNIB.png)
사후확률 (등장하고 나서야 알 수 있다.)
likelihood에 prior를 줘서 클래스간 imbalance한 상태까지도 고려한다.
![](https://i.imgur.com/ESeHEwG.png)
w1x1 + w2x2..
에서 x가 중요했었다
근데 이게 2010 대가 오면서 GPU가 발전돼서 병렬적인 계산의 효율성이 증가하고 nn 모델로 계산가능하게 되었다.
![](https://i.imgur.com/179zCxp.png)
논리회로 연산자 중 하나인 XOR.
![](https://i.imgur.com/KQ7nUo9.png)
여러 feature가 들어갈 때 w를 곱해서 더함
![](https://i.imgur.com/YSp7Y7N.png)
이걸 사용해서 xor모델을 구현할 수 있을까?
못나눔
![](https://i.imgur.com/ov4mFED.png)
decision boundary
xor을 나누기 위해선 선이 line이면 안된다는 결론이 생긴다.
![](https://i.imgur.com/iz3QH1C.png)
여러 레이어에 퍼셉트론을 태우자
퍼셉트론을 여러 겹 태운다는 의미
feedforward nn이라고 하고 mlp라고한다.
![](https://i.imgur.com/7MLpElD.png)
병렬적으로 matrix multiplycation을 GPU쓰면 성능을 끌어올릴 수 있음
![](https://i.imgur.com/udkAmiE.png)
![](https://i.imgur.com/xblM8gI.png)
파란 선이 시그모이드로 0부터 1사이의 값을 가짐.
시그모이드가 0~1이 되는게 불만이라서 만든게 주황색인
하이퍼볼릭 탄젠트 (중간 값이 0이 됨)
요즘은 relu를 씀 가장 중요한건 gradient vanishing을 없애준다는 것이다.

## Evaluation
![](https://i.imgur.com/uLbOooo.png)
classification evaluation은 데이터에 대해서 이야기를 안해볼 수 없는데 데이터셋이라는 거에 종류가 많이 있다.
원하는건 데이터 사용해서 학습시키고 unseen data를 평가한다
근데 unseen이니까 볼 수 없어서
테스트데이터라고 하는것은 unseen데이터를 한거라서 학습과정에 절대로 들어가선 안된다.

loss function에 regulazation을 해준다. w가 너무 커지지 않도록 제약을 걸어준다.
정해야 하는 것은 람다인데 학습을 할 수는 없다.
![](https://i.imgur.com/t3fwYUo.png)
다섯개 다 학습시키고 고른다. (그리드 서치)
0.01이 최고라고 해보자.
best of what?
가지고 있는 것은 두 개의 데이터셋이다.
test data로 고르겠다.. 근데 쓰여선 안된다.

test data은 unseen data를 simulation한다.
![](https://i.imgur.com/q0wp5Yd.png)
온 세상 데이터를 네모라고 하면 네모 데이터를 대표하는 애들이 test dataset이다.
람다를 정할 때 test데이터에 맞춰버리면 test data에 overfit한 데이터가 나와버린다.
그래서 한번 더 잘라버리는데 이걸 val 데이터라고 하고, 테스트 데이터를 모방하는 unseen data라고 할 수 있다.
![](https://i.imgur.com/bpHc8d9.png)
람다와 같은 것들을 하이퍼파라미터를 통해 선택한다.
6:2:2
7:1.5:1.5 정도로 나누는 것이 불문율이라고 한다.
hyperparameter는 인간이 정해줘야 하는 것이다.
![](https://i.imgur.com/G18CKnw.png)
하이퍼 파라미터가 될 수 있는 것들

맞았다 틀렸다가 아니라 다른걸 써야한다.
틀렸다의 정의 방법엔 두가지가 있다.
![](https://i.imgur.com/u2isUDM.png)
recall, precision 둘 다 높아야 높게 나옴
![](https://i.imgur.com/skaXI3J.png)
distance matrix
## Minimum Edit Distance

# 250417 Neural Language Model

![](https://i.imgur.com/HY7LuND.png)
softmax 취해서 단어 갯수만큼 간다.
전체 단어 voca만큼
전체 단어 각각에 대해 확률 예측